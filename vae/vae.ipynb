{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Vision Modeling - Variational Auto Encoders\n",
    "OÄŸuzhan Ercan - x.com/oguzhannercan\n",
    "\n",
    "In this chapter, we will be studying on variational auto encoders. The reason for invastigating on variational auto encoders is going from an image to latent space. The \"Latent Space\" will be discussed in more detailed at DDPM and DDIM sections. In this notebook, we will start to building an Auto Encoder, after that, we will build and Variational Auto Encoder and we will introduce vector quantization to these models. \n",
    "\n",
    "### What is Auto Encoder\n",
    "\n",
    "_A standard Autoencoder (AE) is a neural network architecture composed of two main parts: an encoder and a decoder. The encoder takes an input and compresses it into a lower-dimensional representation called the latent space or code. This compressed representation is then fed into the decoder, which attempts to reconstruct the original input from it. The primary goal of an AE is to learn efficient and useful representations of the input data by minimizing the reconstruction error between the input and the output. - Gemini 2.0_ \n",
    "\n",
    "![Auto Encoder Architecture](media/ae.png)\n",
    "\n",
    "_Figure 1: Auto Encoder Architecture_\n",
    "### What is Variational Auto Encoder\n",
    "\n",
    "_A Variational Autoencoder (VAE) is a generative model that learns a probabilistic distribution over the latent space of the input data. Unlike standard autoencoders that learn a fixed encoding, VAEs encode the input into parameters of a probability distribution, typically a Gaussian. This allows for generating new data points by sampling from this learned latent distribution and decoding it back to the input space. VAEs are particularly useful for tasks like generating realistic images and other complex data. - Gemini 2.0_\n",
    "\n",
    "![Variational Auto Encoder Architecture](media/vae-gaussian.png)\n",
    "\n",
    "_Figure 2: Variational Auto Encoder Architecture_\n",
    "\n",
    "As seen in the figure below, a vae differs from an ae at latent space prediction z, instead of predicting directly z, it assumes the latent space have standart gaussian distrubition, and predicts mean and variance of the sample, then samples a data point from it. Below, we will show the difference between AE and VAE, then we will implement them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Aspect**            | **Autoencoder (AE)**                                   | **Variational Autoencoder (VAE)**                       |\n",
    "|-----------------------|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| **Objective**         | Minimize reconstruction error $$  \\|x - \\hat{x}\\|^2  $$ | Minimize reconstruction error + KL divergence $$  D_{\\text{KL}}(q(z|x) \\| p(z))  $$ |\n",
    "| **Latent Space**      | Deterministic: $$  z = f(x)  $$                         | Probabilistic: $$  z \\sim \\mathcal{N}(\\mu, \\sigma^2)  $$ |\n",
    "| **Loss Function**     | $$  \\mathcal{L} = \\|x - \\hat{x}\\|^2  $$                | $$  \\mathcal{L} = \\|x - \\hat{x}\\|^2 - \\frac{1}{2} \\sum (1 + \\log \\sigma^2 - \\mu^2 - e^{\\log \\sigma^2})  $$ |\n",
    "| **Accuracy**          | High reconstruction fidelity                         | Moderate fidelity due to regularization               |\n",
    "| **Use Cases**         | Data compression, denoising, feature extraction      | Generative modeling, data synthesis, anomaly detection |\n",
    "| **Generative Ability**| No                                                   | Yes, via sampling $$  z \\sim p(z)  $$                    |\n",
    "| **Complexity**        | Simpler optimization                                 | Increased complexity with KL term                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectural Design Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will briefly describe the encoder - decoder architectures.\n",
    "\n",
    "For simplicity, we will first build an Encoder - Decoder architecture with basic convolution layers, after that, we will build stronger architectures that can capture better feature and details.\n",
    "\n",
    "As seen at figure 1, an auto encoder takes a data, in our cases this will be an image. More spesifically we will be working on ImageNet dataset. Encoder applies some transformations, and after these transformations, we can see that the channel size increases and width x height decreases, which is a typical conv2d transformation with appropiate hyper parameters. So we need to decide what will be the shape of encoder's output, which we call latent vector. A typical latent vector for an basic autoencoder is Batch_Size x 512 x 16 x 16, so we will take an image with shape 3 x 64 x 64 (channel x width x height) and convert it the 1x512x1x1.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # bx3x64x64 -> bx64x32x32 \n",
    "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # bx64x32x32 -> bx128x16x16\n",
    "        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        # bx128x16x16 -> bx256x8x8\n",
    "        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        # bx256x32x32 -> bx512x4x4\n",
    "        self.conv4 = nn.Conv2d(256, 512, 4, 2, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(1, 3, 64, 64)\n",
    "encoder = Encoder()\n",
    "print(encoder(tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # bx512x4x4 -> bx256x8x8\n",
    "        self.conv1 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        # bx256x8x8 -> bx128x16x16\n",
    "        self.conv2 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        # bx128x16x16 -> bx64x32x32\n",
    "        self.conv3 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        # bx64x32x32 -> bx3x64x64\n",
    "        self.conv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2)\n",
    "        x = torch.tanh(self.conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(1, 512, 4, 4)\n",
    "decoder = Decoder()\n",
    "print(decoder(tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randn(1, 3, 64, 64)\n",
    "ae = AE()\n",
    "print(ae(tensor).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we build an auto encoder, which first encodes an image with shape 64x64x3 to latent vector with shape 512x4x4, then decodes back it to image space. Now we will train it with a few samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataloader import get_dataloader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import L1Loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
