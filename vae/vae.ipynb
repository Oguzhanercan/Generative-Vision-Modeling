{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Vision Modeling - How to Accually Train Your Variational Auto Encoder\n",
    "OÄŸuzhan Ercan - x.com/oguzhannercan\n",
    "\n",
    "In this chapter, we will be studying on variational auto encoders. The reason for invastigating on variational auto encoders is going from an image to latent space. The \"Latent Space\" will be discussed in more detailed at DDPM and DDIM sections. In this notebook, we will start to building an Auto Encoder, after that, we will build and Variational Auto Encoder and we will introduce vector quantization to these models. \n",
    "\n",
    "### What is Auto Encoder\n",
    "\n",
    "_A standard Autoencoder (AE) is a neural network architecture composed of two main parts: an encoder and a decoder. The encoder takes an input and compresses it into a lower-dimensional representation called the latent space or code. This compressed representation is then fed into the decoder, which attempts to reconstruct the original input from it. The primary goal of an AE is to learn efficient and useful representations of the input data by minimizing the reconstruction error between the input and the output. - Gemini 2.0_ \n",
    "\n",
    "![Auto Encoder Architecture](media/ae.png)\n",
    "\n",
    "_Figure 1: Auto Encoder Architecture_\n",
    "### What is Variational Auto Encoder\n",
    "\n",
    "_A Variational Autoencoder (VAE) is a generative model that learns a probabilistic distribution over the latent space of the input data. Unlike standard autoencoders that learn a fixed encoding, VAEs encode the input into parameters of a probability distribution, typically a Gaussian. This allows for generating new data points by sampling from this learned latent distribution and decoding it back to the input space. VAEs are particularly useful for tasks like generating realistic images and other complex data. - Gemini 2.0_\n",
    "\n",
    "![Variational Auto Encoder Architecture](media/vae-gaussian.png)\n",
    "\n",
    "_Figure 2: Variational Auto Encoder Architecture_\n",
    "\n",
    "As seen in the figure below, a vae differs from an ae at latent space prediction z, instead of predicting directly z, it assumes the latent space have standart gaussian distrubition, and predicts mean and variance of the sample, then samples a data point from it. Below, we will show the difference between AE and VAE, then we will implement them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Aspect**            | **Autoencoder (AE)**                                   | **Variational Autoencoder (VAE)**                       |\n",
    "|-----------------------|-------------------------------------------------------|--------------------------------------------------------|\n",
    "| **Objective**         | Minimize reconstruction error $$  \\|x - \\hat{x}\\|^2  $$ | Minimize reconstruction error + KL divergence $$  D_{\\text{KL}}(q(z|x) \\| p(z))  $$ |\n",
    "| **Latent Space**      | Deterministic: $$  z = f(x)  $$                         | Probabilistic: $$  z \\sim \\mathcal{N}(\\mu, \\sigma^2)  $$ |\n",
    "| **Loss Function**     | $$  \\mathcal{L} = \\|x - \\hat{x}\\|^2  $$                | $$  \\mathcal{L} = \\|x - \\hat{x}\\|^2 - \\frac{1}{2} \\sum (1 + \\log \\sigma^2 - \\mu^2 - e^{\\log \\sigma^2})  $$ |\n",
    "| **Accuracy**          | High reconstruction fidelity                         | Moderate fidelity due to regularization               |\n",
    "| **Use Cases**         | Data compression, denoising, feature extraction      | Generative modeling, data synthesis, anomaly detection |\n",
    "| **Generative Ability**| No                                                   | Yes, via sampling $$  z \\sim p(z)  $$                    |\n",
    "| **Complexity**        | Simpler optimization                                 | Increased complexity with KL term                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectural Design Choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will briefly describe the encoder - decoder architectures.\n",
    "\n",
    "For simplicity, we will first build an Encoder - Decoder architecture with basic convolution layers, after that, we will build stronger architectures that can capture better feature and details.\n",
    "\n",
    "As seen at figure 1, an auto encoder takes a data, in our cases this will be an image. More spesifically we will be working on ImageNet dataset. Encoder applies some transformations, and after these transformations, we can see that the channel size increases and width x height decreases, which is a typical conv2d transformation with appropiate hyper parameters. So we need to decide what will be the shape of encoder's output, which we call latent vector. A typical latent vector for an basic autoencoder is Batch_Size x 512 x 16 x 16, so we will take an image with shape 3 x 64 x 64 (channel x width x height) and convert it the batch_sizex512x4x4.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # bx3x64x64 -> bx64x32x32 \n",
    "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # bx64x32x32 -> bx128x16x16\n",
    "        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        # bx128x16x16 -> bx256x8x8\n",
    "        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        # bx256x32x32 -> bx512x4x4\n",
    "        self.conv4 = nn.Conv2d(256, 512, 4, 2, 1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn4(self.conv4(x)), 0.2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tensor = torch.randn(1, 3, 64, 64)\n",
    "encoder = Encoder()\n",
    "print(encoder(tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # bx512x4x4 -> bx256x8x8\n",
    "        self.conv1 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        # bx256x8x8 -> bx128x16x16\n",
    "        self.conv2 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        # bx128x16x16 -> bx64x32x32\n",
    "        self.conv3 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        # bx64x32x32 -> bx3x64x64\n",
    "        self.conv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2)\n",
    "        x = torch.tanh(self.conv4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tensor = torch.randn(1, 512, 4, 4)\n",
    "decoder = Decoder()\n",
    "print(decoder(tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tensor = torch.randn(1, 3, 64, 64)\n",
    "ae = AE()\n",
    "print(ae(tensor).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we build an auto encoder, which first encodes an image with shape 64x64x3 to latent vector with shape 512x4x4 (which is not making sense, we will discuss this later), then decodes back it to image space. Now we will train it with a few samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import L1Loss\n",
    "def basic_train_config(model,num_train, num_val,batch_size):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    dataset = datasets.ImageFolder(\"/home/oguzhan/Downloads/ImageNetDataset/\", transform=transform)\n",
    "    subset = Subset(dataset, range(num_train))\n",
    "    subset_val = Subset(dataset, range(num_train,num_train+num_val))\n",
    "    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
    "    valloader = DataLoader(subset_val, batch_size=10, shuffle=False)\n",
    "    optimizer = AdamW(model.parameters(), lr=0.0002)\n",
    "    criterion = L1Loss()\n",
    "    return dataloader, valloader, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(model, dataloader, valloader, optimizer, criterion):\n",
    "    outs = []  # MOVED OUTSIDE THE LOOP\n",
    "    for epoch in tqdm(range(20)):\n",
    "        model.train()\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            images = images.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #validate\n",
    "        ae.eval()\n",
    "        if (epoch+1) % 2 == 0:\n",
    "            with torch.no_grad():\n",
    "                for i, (images, _) in enumerate(valloader):\n",
    "                    images = images.to(\"cuda\")\n",
    "                    outputs = ae(images)\n",
    "                    outputs = outputs.cpu().numpy()\n",
    "                    outs.append(outputs)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "ae = AE().to(\"cuda\")\n",
    "dataloader,valloader,optimizer,criterion = basic_train_config(ae,1000,10,32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = train(ae, dataloader, valloader, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage\n",
    "import os\n",
    "def denormalize(tensor):\n",
    "    return (tensor.transpose(1, 2, 0) + 1) * 127.5\n",
    "\n",
    "def get_ground_truth_images(dataloader):\n",
    "    gt_images = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        imgs, _ = batch  # Extract images from batch\n",
    "        imgs = imgs.numpy()\n",
    "        imgs = [denormalize(img) for img in imgs]  # Apply denormalization to each image\n",
    "        \n",
    "        gt_images.extend(imgs)\n",
    "        if len(gt_images) >= 10:  # Stop once we have 10 images\n",
    "            break\n",
    "    \n",
    "    return np.array(gt_images[:10])  \n",
    "\n",
    "#ground_truth = get_ground_truth_images(valloader)\n",
    "\n",
    "\n",
    "def visualize_epoch(epoch_idx):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    plt.suptitle(f'Epoch {(epoch_idx + 1) * 10}')\n",
    "    \n",
    "    predictions = outs[epoch_idx]\n",
    "    \n",
    "    # Verify predictions change\n",
    "    if epoch_idx > 0:\n",
    "        diff = np.mean(np.abs(outs[epoch_idx] - outs[epoch_idx-1]))\n",
    "        print(f\"Mean difference from previous epoch: {diff}\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        gt_img = ground_truth[i]\n",
    "        axes[0, i].imshow(gt_img.astype(np.uint8))\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('Ground Truth')\n",
    "            \n",
    "        pred_img = denormalize(predictions[i])\n",
    "        axes[1, i].imshow(pred_img.astype(np.uint8))\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('Prediction')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(visualize_epoch, \n",
    "         epoch_idx=(0, len(outs)-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the results are blurry. The number of training samples and number of epochs might be limited, but you can trust me about that even we scale training tokens 1 million times, results will be still blurry (I already tried it :). The architecture of autoencoders and training stratgy that we used causes this problem. Solution for blurrines changes from task to task, if we want to train an image to image model, for example takes an image of person then changes features of face (from black hair to blonde hair) we can introduce skip connections from encoder layers to decoder layers. This connections helps our model to retain features relevant to the details. Also architecturel design choices like using batch normalization is causes blurriness but we will not discuss it now. In our case \"Generative Vision Modeling\", our main focus will be sampling a latent vector from a spesific distrubution and generating images from that. If you do not understrand the latest sentence, do not worry about it. We will discuss it later. In this case, we will not be using encoder at all, so we need to come up with different solutions.\n",
    "\n",
    "As you can guess, we will work on variational auto encoders now. We have already discussed the difference between AE and VAE, but here is my most intuitive explanation between them:\n",
    "\n",
    "### _AE's encoder is a compression algorithm - its decoder is the reconstruction model, but VAE's encoder is a image to latent space mapper and its decoder is the conditional image generation model._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        \n",
    "        # bx3x64x64 -> bx64x32x32 \n",
    "        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        # bx64x32x32 -> bx128x16x16\n",
    "        self.conv2 = nn.Conv2d(64, 128, 4, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        # bx128x16x16 -> bx256x8x8\n",
    "        self.conv3 = nn.Conv2d(128, 256, 4, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        # bx256x32x32 -> bx512x4x4\n",
    "        self.conv4 = nn.Conv2d(256, 512, 4, 2, 1)\n",
    "        #mean predictor\n",
    "        self.fc1 = nn.Linear(512*4*4, 256)\n",
    "        #logvar predictor\n",
    "        self.fc2 = nn.Linear(512*4*4, 256)\n",
    "        #latent space\n",
    "        self.fc3 = nn.Linear(256, 512*4*4)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv4(x), 0.2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu = self.fc1(x)\n",
    "        logvar = self.fc2(x)\n",
    "        x = self.fc3(mu)\n",
    "        x = x.view(x.size(0), 512, 4, 4)\n",
    "        x = self.reparameterize(mu, logvar)\n",
    "        return x, mu, logvar\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(256, 512 * 4 * 4)  # Expanding latent vector\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv2 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.conv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)  # Convert (batch_size, 256) -> (batch_size, 512 * 4 * 4)\n",
    "        z = z.view(z.size(0), 512, 4, 4)  # Reshape to 4D tensor for ConvTranspose\n",
    "\n",
    "        z = F.leaky_relu(self.bn1(self.conv1(z)), 0.2)\n",
    "        z = F.leaky_relu(self.bn2(self.conv2(z)), 0.2)\n",
    "        z = F.leaky_relu(self.bn3(self.conv3(z)), 0.2)\n",
    "        pre_activation = self.conv4(z)\n",
    "        print(\"Pre-activation (min, max, mean):\", pre_activation.min().item(), pre_activation.max().item(), pre_activation.mean().item())\n",
    "        z = torch.tanh(pre_activation)\n",
    "\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = VAE_Encoder()\n",
    "decoder = VAE_Decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tensor = torch.randn(1, 3, 64, 64)\n",
    "vae_encoder = VAE_Encoder()\n",
    "x,mu,lovar = vae_encoder(tensor)\n",
    "print(x.shape)\n",
    "print(mu.shape)\n",
    "print(lovar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.encoder = VAE_Encoder()\n",
    "        self.decoder = VAE_Decoder()\n",
    "    def forward(self, x):\n",
    "        x, mu, logvar = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, mu, logvar\n",
    "vae = VAE()\n",
    "tensor = torch.randn(1, 3, 64, 64)\n",
    "x, mu, logvar = vae(tensor)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "vae = VAE()\n",
    "tensor = torch.randn(1, 3, 64, 64)\n",
    "x, mu, logvar = vae(tensor)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(recon_x, x, mu, logvar):\n",
    "    # Reconstruction loss (MSE)\n",
    "    reconstruction_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "# KL Divergence\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return reconstruction_loss + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae(model, dataloader, valloader, optimizer,criterion):\n",
    "    outs = [] \n",
    "    for epoch in tqdm(range(20)):\n",
    "        model.train()\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            images = images.to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            outputs,mu,logvar = model(images)\n",
    "            loss = criterion(outputs, images,mu,logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #validate\n",
    "        ae.eval()\n",
    "        if (epoch+1) % 2 == 0:\n",
    "            with torch.no_grad():\n",
    "                for i, (images, _) in enumerate(valloader):\n",
    "                    images = images.to(\"cuda\")\n",
    "                    outputs = ae(images)\n",
    "                    outputs = outputs.cpu().numpy()\n",
    "                    outs.append(outputs)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.to(\"cuda\")\n",
    "outs = train_vae(vae, dataloader, valloader, optimizer,vae_loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(visualize_epoch, \n",
    "         epoch_idx=(0, len(outs)-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the resulting images are still blurry, and we scale the training tokens 1 million times, again, it will not be clear images. The reasons for that are listed below:\n",
    "\n",
    "- The KLD term in VAEs encourages the latent distribution to be close to a simple prior (usually a Gaussian). If the weight of the KLD term is too high, it can force the model to learn a very smooth and compressed latent space that discards fine details present in the sharp input.\n",
    "\n",
    "- The model might learn a latent representation that captures the general structure but averages out the high-frequency details to minimize the overall pixel error when using L1 or MSE losses. This averaging effect results in a blurry output.\n",
    "\n",
    "- KLD can lead to over-regularization, causing information loss and blurry outputs.\n",
    "\n",
    "\n",
    "For this reason, we will improve our loss function. The table below shows what are these loss terms and their usage purpose.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Loss Function          | Applicable to | Description                                                                                                                              | How it Improves Sharpness                                                                                                                                                                                                                            |\n",
    "| :--------------------- | :------------ | :--------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Perceptual Loss** | AE, VAE       | Compares high-level features from a pre-trained network. Usally we use a pretrained VGG network that trained on Imagenet classfication task.                                                                                   | Encourages sharper outputs by focusing on perceptually important features like edges and textures.                                                                                                                                                  |\n",
    "| **LPIPS** | AE, VAE       | Learned metric for perceptual similarity using pre-trained CNN features.                                                                             | Directly promotes perceptually similar sharp outputs by aligning with human visual perception of image quality.                                                                                                                                  |\n",
    "| **SSIM** | AE, VAE       | Measures structural similarity based on luminance, contrast, and structure.                                                                           | Helps preserve edges and structural details, leading to less blurry and more structurally accurate reconstructions.                                                                                                                               |\n",
    "| **Reconstruction Loss** | AE, VAE       | Pixel-wise difference (e.g., MSE, L1) between output and target.                                                                         | Ensures basic similarity but often needs other losses for sharpness, as it can lead to blurry results by averaging details.                                                                                                                            |\n",
    "| **KLD** | VAE           | Measures the difference between the learned latent distribution and a prior.                                                                        | When balanced, helps learn a meaningful latent space. Over-regularization can cause blurriness; reducing its weight can sometimes improve sharpness.                                                                                              |\n",
    "| **GAN Loss** | AE (hybrid), VAE (hybrid), GAN | Based on a discriminator network that tries to distinguish between real and generated images. The generator tries to fool the discriminator. | Encourages the generator to produce more realistic and sharper images that are indistinguishable from real sharp images to the discriminator. This adversarial process pushes the generator to learn fine details and textures, leading to sharper outputs. |\n",
    "\n",
    "\n",
    "As seen table above, LPIPS ans Perceptual losses seems almost same. Here is the difference between them:\n",
    "\n",
    "| Aspect          | Perceptual Loss                               | LPIPS (Learned Perceptual Image Patch Similarity)                                  |\n",
    "| :--------------- | :-------------------------------------------- | :---------------------------------------------------------------------------------- |\n",
    "| **Definition** | General category of loss functions using pre-trained CNN features. | Specific, learned metric for perceptual similarity using pre-trained CNN features. |\n",
    "| **Learning** | Typically uses a fixed, pre-trained network. | Uses a pre-trained network, but **learns weights** for its feature maps based on human perception data. |\n",
    "| **Feature Weights** | Implicit through the pre-trained network's learned weights (fixed during loss calculation). | Explicitly learns channel-wise weights for feature maps to align with human similarity judgments. |\n",
    "| **Standardization** | Implementation can vary (network, layers, distance). | Provides a more standardized metric with pre-defined (learned) weights.             |\n",
    "| **Output** | A loss value representing the distance between feature maps. | A similarity score (lower is more similar) that correlates with human perceptual similarity. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import lpips\n",
    "from pytorch_msssim import SSIM\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1. Implement Individual Loss Functions ---\n",
    "\n",
    "# Reconstruction Loss \n",
    "def reconstruction_loss(outputs, images, reduction='mean'):\n",
    "    return F.mse_loss(outputs, images, reduction=reduction)\n",
    "\n",
    "# KLD Loss\n",
    "def kld_loss(mu, logvar, reduction='mean'):\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) if reduction == 'sum' else torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "\n",
    "# Perceptual Loss (using VGG)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, layer='relu2_2'):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "        self.selected_layer = layer\n",
    "        self.transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        self.layer_indices = {\n",
    "            'relu1_1': 0,\n",
    "            'relu1_2': 2,\n",
    "            'relu2_1': 5,\n",
    "            'relu2_2': 7,\n",
    "            'relu3_1': 10,\n",
    "            'relu3_2': 12,\n",
    "            'relu3_3': 14,\n",
    "            'relu4_1': 17,\n",
    "            'relu4_2': 19,\n",
    "            'relu4_3': 21,\n",
    "            'relu5_1': 24,\n",
    "            'relu5_2': 26,\n",
    "            'relu5_3': 28,\n",
    "        }\n",
    "\n",
    "        if layer not in self.layer_indices:\n",
    "            raise ValueError(f\"Layer {layer} not found in supported VGG layers.\")\n",
    "\n",
    "        self.selected_layer_index = self.layer_indices[layer]\n",
    "        self.model = vgg16\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "        def get_features(image, model, layer_index):\n",
    "            x = self.transform(image)\n",
    "            features = []\n",
    "            for i, layer in enumerate(model):\n",
    "                x = layer(x)\n",
    "                if i == layer_index:\n",
    "                    features.append(x)\n",
    "                    break\n",
    "            return features\n",
    "\n",
    "        generated_features = get_features(generated, self.model, self.selected_layer_index)\n",
    "        target_features = get_features(target, self.model, self.selected_layer_index)\n",
    "\n",
    "        loss = torch.mean((generated_features[0] - target_features[0]) ** 2)\n",
    "        return loss\n",
    "\n",
    "# LPIPS Loss\n",
    "class LPIPSLoss(nn.Module):\n",
    "    def __init__(self, net='vgg', version='0.1'):\n",
    "        super(LPIPSLoss, self).__init__()\n",
    "        self.lpips_fn = lpips.LPIPS(net=net, version=version).to(device)\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "        return torch.mean(self.lpips_fn(generated, target))\n",
    "\n",
    "# SSIM Loss\n",
    "\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, data_range=1.0, size_average=True, win_size=11, win_sigma=1.5, channel=3):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.ssim = SSIM(data_range=1.0, size_average=True, channel=3).to(device)\n",
    "    def forward(self, img1, img2):\n",
    "        return 1 - self.ssim(img1, img2)\n",
    "    \n",
    "# GAN Loss (for the Generator - VAE's Decoder)\n",
    "class LSGANLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSGANLoss, self).__init__()\n",
    "\n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        # LSGAN uses MSE loss instead of BCE\n",
    "        real_loss = torch.mean((real_output - 1) ** 2)\n",
    "        fake_loss = torch.mean(fake_output ** 2)\n",
    "        return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "    def generator_loss(self, fake_output):\n",
    "        # Generator tries to make fake output close to 1\n",
    "        return torch.mean((fake_output - 1) ** 2)\n",
    "\n",
    "\n",
    "# --- 2. Build a Combined Criterion Function ---\n",
    "\n",
    "def combined_criterion(outputs, images, mu, logvar, loss_config,\n",
    "                      perceptual_criterion=None, lpips_criterion=None, ssim_criterion=None):\n",
    "    total_loss = 0\n",
    "\n",
    "    if 'reconstruction' in loss_config and loss_config['reconstruction']['weight'] > 0:\n",
    "        recon_loss = reconstruction_loss(outputs, images)\n",
    "        total_loss += loss_config['reconstruction']['weight'] * recon_loss\n",
    "\n",
    "    if 'kld' in loss_config and loss_config['kld']['weight'] > 0:\n",
    "        kld = kld_loss(mu, logvar)\n",
    "        total_loss += loss_config['kld']['weight'] * kld\n",
    "\n",
    "    if 'perceptual' in loss_config and loss_config['perceptual']['weight'] > 0:\n",
    "        perc_loss = perceptual_criterion(outputs, images)\n",
    "        total_loss += loss_config['perceptual']['weight'] * perc_loss\n",
    "\n",
    "    if 'lpips' in loss_config and loss_config['lpips']['weight'] > 0:\n",
    "        lpips_loss = lpips_criterion(outputs, images)\n",
    "        total_loss += loss_config['lpips']['weight'] * lpips_loss\n",
    "\n",
    "    if 'ssim' in loss_config and loss_config['ssim']['weight'] > 0:\n",
    "        ssim_loss = ssim_criterion(outputs, images)\n",
    "        total_loss += loss_config['ssim']['weight'] * ssim_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Define a simple Discriminator for GAN loss\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        \n",
    "        def conv_block(in_channels, out_channels, kernel_size=4, stride=2, padding=1, use_bn=True):\n",
    "            layers = [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "            if use_bn:\n",
    "                layers.insert(1, nn.InstanceNorm2d(out_channels))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # PatchGAN-style discriminator with multiple output patches\n",
    "        self.conv1 = conv_block(input_channels, 64, use_bn=False)  # 64x32x32\n",
    "        self.conv2 = conv_block(64, 128)                         # 128x16x16\n",
    "        self.conv3 = conv_block(128, 256)                        # 256x8x8\n",
    "        self.conv4 = conv_block(256, 512)                        # 512x4x4\n",
    "        self.conv5 = nn.Conv2d(512, 1, 4, 1, 0)                 # 1x1x1 (patch outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return torch.sigmoid(x)  # Output patch-wise probabilities\n",
    "\n",
    "\n",
    "# --- 3. Modify the Training Function ---\n",
    "def train_vae(model, dataloader, valloader, optimizer, loss_config, discriminator=None, \n",
    "              discriminator_optimizer=None, num_epochs=20,\n",
    "              perceptual_criterion=None, lpips_criterion=None, ssim_criterion=None, \n",
    "              run_name=\"default_run\"):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gan_loss_fn = LSGANLoss()\n",
    "    \n",
    "    output_root_dir = f\"jn_outs/{run_name}\"\n",
    "    gt_dir = os.path.join(output_root_dir, \"ground_truth\")\n",
    "    os.makedirs(gt_dir, exist_ok=True)\n",
    "    os.makedirs(output_root_dir, exist_ok=True)\n",
    "    to_pil = ToPILImage()\n",
    "    ground_truth_saved = False\n",
    "\n",
    "    # Save ground truth images\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, _) in enumerate(valloader):\n",
    "            if not ground_truth_saved:\n",
    "                images_denorm = denormalize(images).cpu()\n",
    "                for i in range(min(10, images_denorm.size(0))):\n",
    "                    img_tensor = images_denorm[i]\n",
    "                    img_pil = to_pil(img_tensor)\n",
    "                    img_pil.save(os.path.join(gt_dir, f\"gt_image_{batch_idx * valloader.batch_size + i}.png\"))\n",
    "                if (batch_idx + 1) * valloader.batch_size >= 10:\n",
    "                    ground_truth_saved = True\n",
    "                    break\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        for i, (images, _) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # VAE/Generator forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs, mu, logvar = model(images)\n",
    "            \n",
    "            # Calculate combined loss excluding GAN\n",
    "            total_loss = combined_criterion(outputs, images, mu, logvar, loss_config,\n",
    "                                          perceptual_criterion=perceptual_criterion,\n",
    "                                          lpips_criterion=lpips_criterion,\n",
    "                                          ssim_criterion=ssim_criterion)\n",
    "\n",
    "            # GAN training if enabled\n",
    "            if 'gan' in loss_config and loss_config['gan']['weight'] > 0 and discriminator is not None:\n",
    "                # Discriminator training\n",
    "                discriminator_optimizer.zero_grad()\n",
    "                real_output = discriminator(images)\n",
    "                fake_output = discriminator(outputs.detach())\n",
    "                d_loss = gan_loss_fn.discriminator_loss(real_output, fake_output)\n",
    "                d_loss.backward()\n",
    "                discriminator_optimizer.step()\n",
    "\n",
    "                # Generator GAN loss\n",
    "                fake_output = discriminator(outputs)\n",
    "                g_gan_loss = gan_loss_fn.generator_loss(fake_output)\n",
    "                total_loss += loss_config['gan']['weight'] * g_gan_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation and saving\n",
    "        model.eval()\n",
    "        if epoch % 2 == 0:\n",
    "            with torch.no_grad():\n",
    "                epoch_output_dir = os.path.join(output_root_dir, f\"epoch_{epoch}\")\n",
    "                os.makedirs(epoch_output_dir, exist_ok=True)\n",
    "                for i, (images, _) in enumerate(valloader):\n",
    "                    images = images.to(device)\n",
    "                    outputs, _, _ = model(images)\n",
    "                    outputs_denorm = denormalize(outputs).cpu()\n",
    "                    for j in range(outputs_denorm.size(0)):\n",
    "                        img_tensor = outputs_denorm[j]\n",
    "                        img_pil = to_pil(img_tensor)\n",
    "                        img_pil.save(os.path.join(epoch_output_dir, f\"prediction_{i * valloader.batch_size + j}.png\"))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os \n",
    "input_channels = 3\n",
    "image_size = 64\n",
    "input_dim = input_channels * image_size * image_size\n",
    "vae = VAE().to(device)\n",
    "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "discriminator = PatchDiscriminator(input_channels=3).to(device)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "perceptual_criterion = PerceptualLoss(layer='relu3_3').to(device)\n",
    "lpips_criterion = LPIPSLoss(net='vgg').to(device)\n",
    "ssim_criterion = SSIMLoss(data_range=1.0).to(device)\n",
    "dataloader,valloader,_,_ = basic_train_config(vae,5000,10,256)\n",
    "\n",
    "# Define the loss configuration\n",
    "loss_config = {\n",
    "    'reconstruction': {'weight': 1.0},\n",
    "    'kld': {'weight': 0.01},\n",
    "    'perceptual': {'weight': 0.1, 'layer': 'relu3_3'},\n",
    "    'lpips': {'weight': 0.5, 'net': 'vgg'},\n",
    "    'ssim': {'weight': 0.2},\n",
    "    'gan': {'weight': 0.05},\n",
    "}\n",
    "\n",
    "# Train the VAE with the combined criterion\n",
    "train_vae(vae, dataloader, valloader, vae_optimizer, loss_config, discriminator, discriminator_optimizer,\n",
    "                               num_epochs=10,\n",
    "                               perceptual_criterion=perceptual_criterion, # Pass the initialized objects\n",
    "                               lpips_criterion=lpips_criterion,\n",
    "                               ssim_criterion=ssim_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "### Here, we are updating visualization function so it can read from the output directory\n",
    "def denormalize(tensor):\n",
    "    return (tensor + 1) / 2  # Assuming your normalization was to [-1, 1]\n",
    "\n",
    "def visualize_epoch(epoch_idx, run_name=\"default_run\"):\n",
    "    print(epoch_idx)\n",
    "    output_dir = f\"jn_outs/{run_name}/epoch_{epoch_idx * 2}\" # Outputs saved every 2 epochs\n",
    "    gt_dir = f\"jn_outs/{run_name}/ground_truth\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Error: Prediction directory not found at {output_dir}. Please run training first.\")\n",
    "        return\n",
    "    if not os.path.exists(gt_dir):\n",
    "        print(f\"Error: Ground truth directory not found at {gt_dir}. Please run training first.\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    plt.suptitle(f'Epoch {(epoch_idx ) * 2}')\n",
    "\n",
    "    for i in range(5):\n",
    "        gt_filename = os.path.join(gt_dir, f\"gt_image_{i}.png\")\n",
    "        if os.path.exists(gt_filename):\n",
    "            gt_img = PILImage.open(gt_filename)\n",
    "            axes[0, i].imshow(gt_img)\n",
    "            axes[0, i].axis('off')\n",
    "            if i == 0:\n",
    "                axes[0, i].set_title('Ground Truth')\n",
    "        else:\n",
    "            axes[0, i].axis('off')\n",
    "            axes[0, i].text(0.5, 0.5, 'GT\\nNot Found', ha='center', va='center')\n",
    "\n",
    "        pred_filename = os.path.join(output_dir, f\"prediction_{i}.png\")\n",
    "        if os.path.exists(pred_filename):\n",
    "            pred_img = PILImage.open(pred_filename)\n",
    "            axes[1, i].imshow(pred_img)\n",
    "            axes[1, i].axis('off')\n",
    "            if i == 0:\n",
    "                axes[1, i].set_title('Prediction')\n",
    "        else:\n",
    "            axes[1, i].axis('off')\n",
    "            axes[1, i].text(0.5, 0.5, 'Prediction\\nNot Found', ha='center', va='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage\n",
    "import os\n",
    "num_epochs = 8 # Example\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "# Calculate the number of saved epochs\n",
    "num_saved_epochs = num_epochs // 2 if num_epochs % 2 == 0 else num_epochs // 2 + 1\n",
    "print(num_saved_epochs)\n",
    "interact(visualize_epoch, epoch_idx=(0, num_saved_epochs , 1),run_name = \"25epoch_basic_vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training results above shows that we are facing a common issue caused by GAN loss. The output appears to be a noisy, grid-like pattern with no discernible structure, which suggests that the generator (in this case, the VAE's decoder) is failing to produce meaningful images. This is often a symptom of poor loss balancing, mode collapse, or issues with the training dynamics between the generator and discriminator. Below, the loss configuration is noted:\n",
    "\n",
    "- Reconstruction Loss (MSE): Weight = 1.0\n",
    "- KL Divergence (KLD): Weight = 0.01\n",
    "- Perceptual Loss: Weight = 0.1\n",
    "- LPIPS Loss: Weight = 0.5\n",
    "- SSIM Loss: Weight = 0.2\n",
    "- GAN Loss: Weight = 0.05\n",
    "\n",
    "\n",
    "To fix this issue, we will balance the and try to train it again. Here is an important insight:\n",
    "\n",
    "### _One of the most challenging part of training a generative vision model is balancing the loss function._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "input_channels = 3\n",
    "image_size = 64\n",
    "input_dim = input_channels * image_size * image_size\n",
    "vae = VAE().to(device)\n",
    "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "discriminator = PatchDiscriminator(input_channels=3).to(device)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "perceptual_criterion = PerceptualLoss(layer='relu3_3').to(device)\n",
    "lpips_criterion = LPIPSLoss(net='vgg').to(device)\n",
    "ssim_criterion = SSIMLoss(data_range=1.0).to(device)\n",
    "dataloader,valloader,_,_ = basic_train_config(vae,5000,10,256)\n",
    "\n",
    "# Define the loss configuration\n",
    "loss_config = {\n",
    "    'reconstruction': {'weight': 0.1},  # Reduce the weight of MSE\n",
    "    'kld': {'weight': 0.05},           # Slightly increase KLD for better latent space regularization\n",
    "    'perceptual': {'weight': 0.5},     # Increase perceptual loss to focus on high-level features\n",
    "    'lpips': {'weight': 0.5},          # Keep LPIPS as is\n",
    "    'ssim': {'weight': 0.2},           # Keep SSIM as is\n",
    "    'gan': {'weight': 1.0},            # Significantly increase GAN loss to encourage realistic outputs\n",
    "}\n",
    "\n",
    "# Train the VAE with the combined criterion\n",
    "train_vae(vae, dataloader, valloader, vae_optimizer, loss_config, discriminator, discriminator_optimizer,\n",
    "                               num_epochs=10,\n",
    "                               perceptual_criterion=perceptual_criterion, # Pass the initialized objects\n",
    "                               lpips_criterion=lpips_criterion,\n",
    "                               ssim_criterion=ssim_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage\n",
    "import os\n",
    "num_epochs = 25 # Example\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "# Calculate the number of saved epochs\n",
    "num_saved_epochs = num_epochs // 2 if num_epochs % 2 == 0 else num_epochs // 2 + 1\n",
    "print(num_saved_epochs)\n",
    "interact(visualize_epoch, epoch_idx=(0, num_saved_epochs , 1),run_name = \"25epoch_basic_vae_loss_balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, as we can see, all of the images are black. Now we need to fix this issue, The issue of black outputs (likely all pixels being close to 0 after denormalization, resulting in a black image) from the first epoch after the loss balancing update, compared to after the 10th epoch before the update, suggests a problem in the training dynamics or the loss computation that causes the generator (VAE's decoder) to collapse its output to a trivial solution (e.g., all zeros). \n",
    "\n",
    "Since we use tanh (it is just a design choice, we could choose sigmoid etc.) as the last layer of decoder, the output range is [-1,1], and our denormalization function is formulated as denormalized = (tensor + 1) / 2. The black output shows that our values are all zeros. And if you look at the results of our first experiment, you can see that last saved epoch's results are also black, suggesting a gradual collapse of the generator's output. After the update, the outputs are black from the first epoch, indicating that the changes we made (e.g., rebalancing the loss weights, adding gradient penalty, label smoothing, etc.) have accelerated this collapse. Lets analyze our changes:\n",
    "\n",
    "- The reconstruction loss weight was reduced from 1.0 to 0.1, meaning the generator is less incentivized to match the input pixel-wise. While this was intended to reduce blurriness, it might have removed a stabilizing force that was keeping the generatorâ€™s output from collapsing.\n",
    "- The GAN loss weight was increased from 0.05 to 1.0, giving the adversarial loss a much stronger influence. If the discriminator becomes too strong too quickly, it can overpower the generator, causing the generator to fail to produce meaningful outputs.\n",
    "- The KLD weight was increased from 0.01 to 0.05, which might impose stronger regularization on the latent space, potentially constraining the generatorâ€™s ability to explore diverse outputs.\n",
    "\n",
    "As I said before, loss balancing is an important problem, here is the current loss contribition of each criterion:\n",
    "\n",
    "Epoch 1/25:  12%| | 45/390 00:40<05:17,  1.09it/s, Recon=0.6670, KLD=2566.3564, Perceptual=606.7762, LPIPS=0.7647, SSIM=0.9983, GAN_G=0.8666,\n",
    "\n",
    "\n",
    "and after a few epochs:\n",
    "\n",
    "Epoch 4/25:  47%|â–ˆâ–‰  | 183/390 [02:34<02:52,  1.20it/s, Recon=nan, KLD=nan, Perceptual=nan, LPIPS=nan, SSIM=nan, GAN_G=nan, GAN_D=nan, Total=nan]\n",
    "\n",
    "\n",
    "KLD loss gradually icreased and became nan, after that, all of the training progress has collapsed. The reason for that is logvar term gets bigger and bigger. And since the formulation of kld is includes exponent of logvar, it did not fit into float 32. We will make sure that logvar does not get to big. \n",
    "\n",
    "We will apply things below:\n",
    "\n",
    "- Lowering the discriminators learning speed (lower learning rate ): Giving flexible time for generator to output a little bit realistic images so it can compete with discriminator\n",
    "- Increasing label smoothing: We will increase it because as we discussed, we want to help generator.  \n",
    "- Increasing reconstruction weight: We can say that reconstruction is important because we want an output image which is identical with input image, since we decreased it and see worse results, it is somehow helps model to collapse in a local minima which is close to initial point.\n",
    "- KLD Loss Warmup: To prevent the KLD loss from dominating early in training, we will add a warm-up phase where the KLD weight starts at 0 and gradually increases to its target value over the first few epochs.\n",
    "- Architectural changes for stability: For some reason I do not know , mu and logvar becomes extremly huge for this training script, huge logvar values are common issue at vae training, but mostly it effects the loss calculation but in this trainign script, it breaks numerical stability before loss calculation, at reparametrization stage. With some experiments, I can say that replacing batch norm with group norm and changing conv kernel size 4 -> 3 fixes this issue. \n",
    "- Logvar and mu clampling: To prevent huge KLD loss, we will clamp logvar and mu if they exceed -20 or 20.\n",
    "\n",
    "\n",
    "Below, there will be some duplicated codes, because I want to show our updates clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Whole imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "import lpips\n",
    "from pytorch_msssim import SSIM\n",
    "from tqdm import tqdm\n",
    "from torchvision.transforms import ToPILImage, transforms\n",
    "from PIL import Image as PILImage\n",
    "import os \n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import L1Loss\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    \"\"\"Downsampling layer using convolution with stride 2.\"\"\"\n",
    "    def __init__(self, in_channels: int,out_channels: int = None):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pad = (0, 1, 0, 1)  # Pad to maintain spatial dims before strided conv\n",
    "        x = nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        \n",
    "        # bx3x64x64 -> bx64x32x32 \n",
    "        self.conv1 = Downsample(3,64)\n",
    "        self.gn1 = nn.GroupNorm(num_groups=32, num_channels=64)  # 64 channels, 32 groups\n",
    "        # bx64x32x32 -> bx128x16x16\n",
    "        self.conv2 = Downsample(64,128)\n",
    "        self.gn2 = nn.GroupNorm(num_groups=32, num_channels=128)  # 128 channels, 32 groups\n",
    "        # bx128x16x16 -> bx256x8x8\n",
    "        self.conv3 = Downsample(128,256)\n",
    "        self.gn3 = nn.GroupNorm(num_groups=32, num_channels=256)  # 256 channels, 32 groups\n",
    "        # bx256x32x32 -> bx512x4x4\n",
    "        self.conv4 = Downsample(256,512)\n",
    "        # mean predictor\n",
    "        self.fc1 = nn.Linear(512*4*4, 256)\n",
    "        # logvar predictor\n",
    "        self.fc2 = nn.Linear(512*4*4, 256)\n",
    "\n",
    "        # Initialize weights with smaller variance\n",
    "        nn.init.normal_(self.fc1.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.fc2.weight, mean=0.0, std=0.02)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.gn1(self.conv1(x)), 0.2)\n",
    "        x = F.leaky_relu(self.gn2(self.conv2(x)), 0.2)\n",
    "        x = F.leaky_relu(self.gn3(self.conv3(x)), 0.2)\n",
    "        x = F.leaky_relu(self.conv4(x), 0.2)\n",
    "        x = x.view(x.size(0), -1)  # Shape: (B, 512*4*4)\n",
    "        mu = self.fc1(x)           # Shape: (B, 256)\n",
    "        logvar = self.fc2(x)       # Shape: (B, 256)\n",
    "        # Use clamped values for reparameterization\n",
    "        #mu_clamped = torch.clamp(mu, min=-10, max=10)\n",
    "        #logvar_clamped = torch.clamp(logvar, min=-10, max=10)\n",
    "        z = self.reparameterize(mu, logvar)  # Shape: (B, 256)\n",
    "        return z, mu,logvar#mu_clamped, logvar_clamped\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(256, 512 * 4 * 4)  # Expanding latent vector\n",
    "\n",
    "        self.conv1 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n",
    "        self.gn1 = nn.GroupNorm(num_groups=32, num_channels=256)  # 256 channels, 32 groups\n",
    "\n",
    "        self.conv2 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
    "        self.gn2 = nn.GroupNorm(num_groups=32, num_channels=128)  # 128 channels, 32 groups\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
    "        self.gn3 = nn.GroupNorm(num_groups=32, num_channels=64)  # 64 channels, 32 groups\n",
    "\n",
    "        self.conv4 = nn.ConvTranspose2d(64, 3, 4, 2, 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)  # Convert (batch_size, 256) -> (batch_size, 512 * 4 * 4)\n",
    "        z = z.view(z.size(0), 512, 4, 4)  # Reshape to 4D tensor for ConvTranspose\n",
    "\n",
    "        z = F.leaky_relu(self.gn1(self.conv1(z)), 0.2)\n",
    "        z = F.leaky_relu(self.gn2(self.conv2(z)), 0.2)\n",
    "        z = F.leaky_relu(self.gn3(self.conv3(z)), 0.2)\n",
    "        pre_activation = self.conv4(z)\n",
    "        print(\"Pre-activation (min, max, mean):\", pre_activation.min().item(), pre_activation.max().item(), pre_activation.mean().item())\n",
    "        z = torch.tanh(pre_activation)\n",
    "\n",
    "        return z\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        self.encoder = VAE_Encoder()\n",
    "        self.decoder = VAE_Decoder()\n",
    "    def forward(self, x):\n",
    "        x, mu, logvar = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(outputs, images, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Compute the reconstruction loss (MSE) between outputs and images.\n",
    "    Args:\n",
    "        outputs: Tensor of shape (B, C, H, W)\n",
    "        images: Tensor of shape (B, C, H, W)\n",
    "        reduction: 'mean' (per-sample) or 'sum' (batch-level)\n",
    "    Returns:\n",
    "        Per-sample loss if reduction='mean', batch-level loss if reduction='sum'\n",
    "    \"\"\"\n",
    "    return F.mse_loss(outputs, images, reduction=reduction)\n",
    "\n",
    "# KLD Loss\n",
    "def kld_loss(mu, logvar, reduction='mean'):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence loss.\n",
    "    Args:\n",
    "        mu: Tensor of shape (B, latent_dim)\n",
    "        logvar: Tensor of shape (B, latent_dim)\n",
    "        reduction: 'mean' (per-sample) or 'sum' (batch-level)\n",
    "    Returns:\n",
    "        Per-sample loss if reduction='mean', batch-level loss if reduction='sum'\n",
    "    \"\"\"\n",
    "    mu = torch.clamp(mu, min=-20, max=20)\n",
    "    logvar = torch.clamp(logvar, min=-20, max=20)  # Prevent numerical instability\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    return kld.sum() if reduction == 'sum' else kld.mean()\n",
    "\n",
    "# Perceptual Loss (with proper normalization)\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, layer='relu3_3'):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg16 = models.vgg16(pretrained=True).features.to(device).eval()\n",
    "        self.selected_layer = layer\n",
    "        self.transform = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "        self.layer_indices = {\n",
    "            'relu1_1': 0, 'relu1_2': 2, 'relu2_1': 5, 'relu2_2': 7,\n",
    "            'relu3_1': 10, 'relu3_2': 12, 'relu3_3': 14,\n",
    "            'relu4_1': 17, 'relu4_2': 19, 'relu4_3': 21,\n",
    "            'relu5_1': 24, 'relu5_2': 26, 'relu5_3': 28,\n",
    "        }\n",
    "\n",
    "        if layer not in self.layer_indices:\n",
    "            raise ValueError(f\"Layer {layer} not found in supported VGG layers.\")\n",
    "\n",
    "        self.selected_layer_index = self.layer_indices[layer]\n",
    "        self.model = vgg16\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "        # Denormalize from [-1, 1] to [0, 1]\n",
    "        generated = (generated + 1) / 2\n",
    "        target = (target + 1) / 2\n",
    "        # Apply VGG normalization\n",
    "        generated = self.transform(generated)\n",
    "        target = self.transform(target)\n",
    "\n",
    "        def get_features(image, model, layer_index):\n",
    "            x = image\n",
    "            for i, layer in enumerate(model):\n",
    "                x = layer(x)\n",
    "                if i == layer_index:\n",
    "                    return x\n",
    "            return x\n",
    "\n",
    "        generated_features = get_features(generated, self.model, self.selected_layer_index)\n",
    "        target_features = get_features(target, self.model, self.selected_layer_index)\n",
    "\n",
    "        loss = torch.mean((generated_features - target_features) ** 2)\n",
    "        return loss\n",
    "\n",
    "# LPIPS Loss\n",
    "class LPIPSLoss(nn.Module):\n",
    "    def __init__(self, net='vgg', version='0.1'):\n",
    "        super(LPIPSLoss, self).__init__()\n",
    "        self.lpips_fn = lpips.LPIPS(net=net, version=version).to(device)\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "        return torch.mean(self.lpips_fn(generated, target))\n",
    "\n",
    "# SSIM Loss\n",
    "\n",
    "\n",
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self, data_range=1.0, size_average=True, win_size=11, win_sigma=1.5, channel=3):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.ssim = SSIM(data_range=1.0, size_average=True, channel=3).to(device)\n",
    "    def forward(self, img1, img2):\n",
    "        return 1 - self.ssim(img1, img2)\n",
    "    \n",
    "# GAN Loss (for the Generator - VAE's Decoder)\n",
    "class LSGANLoss(nn.Module):\n",
    "    def __init__(self, label_smoothing=0.2):  # Increased label smoothing\n",
    "        super(LSGANLoss, self).__init__()\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_target = 1.0 - self.label_smoothing  # e.g., 0.8\n",
    "        fake_target = 0.0 + self.label_smoothing  # e.g., 0.2\n",
    "        real_loss = torch.mean((real_output - real_target) ** 2)\n",
    "        fake_loss = torch.mean((fake_output - fake_target) ** 2)\n",
    "        return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "    def generator_loss(self, fake_output):\n",
    "        return torch.mean((fake_output - (1.0 - self.label_smoothing)) ** 2)\n",
    "\n",
    "\n",
    "# --- 2. Build a Combined Criterion Function ---\n",
    "\n",
    "def combined_criterion(outputs, images, mu, logvar, loss_config, kld_weight,\n",
    "                      perceptual_criterion=None, lpips_criterion=None, ssim_criterion=None):\n",
    "    total_loss = 0\n",
    "\n",
    "    if 'reconstruction' in loss_config and loss_config['reconstruction']['weight'] > 0:\n",
    "        recon_loss = reconstruction_loss(outputs, images)\n",
    "        total_loss += loss_config['reconstruction']['weight'] * recon_loss\n",
    "\n",
    "    if 'kld' in loss_config and loss_config['kld']['weight'] > 0:\n",
    "        kld = kld_loss(mu, logvar)\n",
    "        total_loss += kld_weight * kld  # Use dynamic KLD weight\n",
    "\n",
    "    if 'perceptual' in loss_config and loss_config['perceptual']['weight'] > 0:\n",
    "        perc_loss = perceptual_criterion(outputs, images)\n",
    "        total_loss += loss_config['perceptual']['weight'] * perc_loss\n",
    "\n",
    "    if 'lpips' in loss_config and loss_config['perceptual']['weight'] > 0:\n",
    "        lpips_loss = lpips_criterion(outputs, images)\n",
    "        total_loss += loss_config['lpips']['weight'] * lpips_loss\n",
    "\n",
    "    if 'ssim' in loss_config and loss_config['ssim']['weight'] > 0:\n",
    "        ssim_loss = ssim_criterion(outputs, images)\n",
    "        total_loss += loss_config['ssim']['weight'] * ssim_loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "        \n",
    "        def conv_block(in_channels, out_channels, kernel_size=4, stride=2, padding=1, use_bn=True):\n",
    "            layers = [\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            ]\n",
    "            if use_bn:\n",
    "                layers.insert(1, nn.InstanceNorm2d(out_channels))\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        # PatchGAN-style discriminator with multiple output patches\n",
    "        self.conv1 = conv_block(input_channels, 64, use_bn=False)  # 64x32x32\n",
    "        self.conv2 = conv_block(64, 128)                         # 128x16x16\n",
    "        self.conv3 = conv_block(128, 256)                        # 256x8x8\n",
    "        self.conv4 = conv_block(256, 512)                        # 512x4x4\n",
    "        self.conv5 = nn.Conv2d(512, 1, 4, 1, 0)                 # 1x1x1 (patch outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return torch.sigmoid(x)  # Output patch-wise probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Modify the Training Function ---\n",
    "def train_vae(model, dataloader, valloader, optimizer, loss_config, discriminator=None, \n",
    "              discriminator_optimizer=None, num_epochs=20,\n",
    "              perceptual_criterion=None, lpips_criterion=None, ssim_criterion=None, \n",
    "              run_name=\"default_run\"):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gan_loss_fn = LSGANLoss(label_smoothing=0.2)  # Increased label smoothing\n",
    "    \n",
    "    output_root_dir = f\"jn_outs/{run_name}\"\n",
    "    gt_dir = os.path.join(output_root_dir, \"ground_truth\")\n",
    "    os.makedirs(gt_dir, exist_ok=True)\n",
    "    os.makedirs(output_root_dir, exist_ok=True)\n",
    "    to_pil = ToPILImage()\n",
    "    ground_truth_saved = False\n",
    "\n",
    "    # Save ground truth images\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, _) in enumerate(valloader):\n",
    "            if not ground_truth_saved:\n",
    "                images_denorm = denormalize(images).cpu()\n",
    "                for i in range(min(10, images_denorm.size(0))):\n",
    "                    img_tensor = images_denorm[i]\n",
    "                    img_pil = to_pil(img_tensor)\n",
    "                    img_pil.save(os.path.join(gt_dir, f\"gt_image_{batch_idx * valloader.batch_size + i}.png\"))\n",
    "                if (batch_idx + 1) * valloader.batch_size >= 10:\n",
    "                    ground_truth_saved = True\n",
    "                    break\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "        \n",
    "        # Compute dynamic KLD weight for warm-up (over first 5 epochs)\n",
    "        kld_weight = max(0.00001, min(loss_config['kld']['weight'] * (epoch / 20.0), loss_config['kld']['weight']))  # Minimum KLD weight of 0.00001\n",
    "        \n",
    "        # Accumulate losses for averaging over the epoch\n",
    "        epoch_recon_loss = 0.0\n",
    "        epoch_kld_loss = 0.0\n",
    "        epoch_perceptual_loss = 0.0\n",
    "        epoch_lpips_loss = 0.0\n",
    "        epoch_ssim_loss = 0.0\n",
    "        epoch_gan_g_loss = 0.0\n",
    "        epoch_gan_d_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for i, (images, _) in enumerate(train_loop):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # VAE/Generator forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs, mu, logvar = model(images)\n",
    "            print(f\"Batch {i+1} - mu (min, max, mean): {mu.min().item():.4f}, {mu.max().item():.4f}, {mu.mean().item():.4f}\")\n",
    "            print(f\"Batch {i+1} - logvar (min, max, mean): {logvar.min().item():.4f}, {logvar.max().item():.4f}, {logvar.mean().item():.4f}\")\n",
    "            # Compute individual losses\n",
    "            recon_loss = reconstruction_loss(outputs, images) if 'reconstruction' in loss_config else 0.0\n",
    "            kld = kld_loss(mu, logvar) if 'kld' in loss_config else 0.0\n",
    "            perc_loss = perceptual_criterion(outputs, images) if 'perceptual' in loss_config and perceptual_criterion else 0.0\n",
    "            lpips_loss = lpips_criterion(outputs, images) if 'lpips' in loss_config and lpips_criterion else 0.0\n",
    "            ssim_loss = ssim_criterion(outputs, images) if 'ssim' in loss_config and ssim_criterion else 0.0\n",
    "\n",
    "            # Calculate combined loss excluding GAN\n",
    "            total_loss = combined_criterion(outputs, images, mu, logvar, loss_config, kld_weight,\n",
    "                                            perceptual_criterion=perceptual_criterion,\n",
    "                                            lpips_criterion=lpips_criterion,\n",
    "                                            ssim_criterion=ssim_criterion)\n",
    "\n",
    "            # GAN training if enabled\n",
    "            n_discriminator_steps = 1\n",
    "            g_gan_loss = 0.0\n",
    "            d_loss = 0.0\n",
    "            if 'gan' in loss_config and loss_config['gan']['weight'] > 0 and discriminator is not None:\n",
    "                for _ in range(n_discriminator_steps):\n",
    "                    discriminator_optimizer.zero_grad()\n",
    "                    real_output = discriminator(images)\n",
    "                    fake_output = discriminator(outputs.detach())\n",
    "                    d_loss = gan_loss_fn.discriminator_loss(real_output, fake_output)\n",
    "                    d_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)  # Add gradient clipping\n",
    "                    discriminator_optimizer.step()\n",
    "\n",
    "                # Generator GAN loss\n",
    "                fake_output = discriminator(outputs)\n",
    "                g_gan_loss = gan_loss_fn.generator_loss(fake_output)\n",
    "                total_loss += loss_config['gan']['weight'] * g_gan_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Add gradient clipping\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate losses for epoch averaging\n",
    "            epoch_recon_loss += recon_loss.item()\n",
    "            epoch_kld_loss += kld.item()\n",
    "            epoch_perceptual_loss += perc_loss.item() if perc_loss != 0 else 0.0\n",
    "            epoch_lpips_loss += lpips_loss.item() if lpips_loss != 0 else 0.0\n",
    "            epoch_ssim_loss += ssim_loss.item() if ssim_loss != 0 else 0.0\n",
    "            epoch_gan_g_loss += g_gan_loss.item() if g_gan_loss != 0 else 0.0\n",
    "            epoch_gan_d_loss += d_loss.item() if d_loss != 0 else 0.0\n",
    "            num_batches += 1\n",
    "\n",
    "            # Update tqdm progress bar with current batch losses (all losses are per-sample, normalized by batch size)\n",
    "            train_loop.set_postfix({\n",
    "                'Recon': f'{recon_loss.item():.4f}',\n",
    "                'KLD': f'{kld.item():.4f}',\n",
    "                'Perceptual': f'{perc_loss.item():.4f}' if perc_loss != 0 else 'N/A',\n",
    "                'LPIPS': f'{lpips_loss.item():.4f}' if lpips_loss != 0 else 'N/A',\n",
    "                'SSIM': f'{ssim_loss.item():.4f}' if ssim_loss != 0 else 'N/A',\n",
    "                'GAN_G': f'{g_gan_loss.item():.4f}' if g_gan_loss != 0 else 'N/A',\n",
    "                'GAN_D': f'{d_loss.item():.4f}' if d_loss != 0 else 'N/A',\n",
    "                'Total': f'{total_loss.item():.4f}',\n",
    "                'KLD_Weight': f'{kld_weight:.4f}'  # Display the current KLD weight\n",
    "            })\n",
    "\n",
    "        # Compute average losses for the epoch (all averages are per-sample)\n",
    "        avg_recon_loss = epoch_recon_loss / num_batches\n",
    "        avg_kld_loss = epoch_kld_loss / num_batches\n",
    "        avg_perceptual_loss = epoch_perceptual_loss / num_batches\n",
    "        avg_lpips_loss = epoch_lpips_loss / num_batches\n",
    "        avg_ssim_loss = epoch_ssim_loss / num_batches\n",
    "        avg_gan_g_loss = epoch_gan_g_loss / num_batches\n",
    "        avg_gan_d_loss = epoch_gan_d_loss / num_batches\n",
    "\n",
    "        # Print average losses for the epoch\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} - Average Losses:\")\n",
    "        print(f\"  Reconstruction: {avg_recon_loss:.4f}\")\n",
    "        print(f\"  KLD: {avg_kld_loss:.4f}\")\n",
    "        print(f\"  Perceptual: {avg_perceptual_loss:.4f}\")\n",
    "        print(f\"  LPIPS: {avg_lpips_loss:.4f}\")\n",
    "        print(f\"  SSIM: {avg_ssim_loss:.4f}\")\n",
    "        print(f\"  GAN (Generator): {avg_gan_g_loss:.4f}\")\n",
    "        print(f\"  GAN (Discriminator): {avg_gan_d_loss:.4f}\\n\")\n",
    "\n",
    "        # Validation and saving\n",
    "        model.eval()\n",
    "        if epoch % 2 == 0:\n",
    "            with torch.no_grad():\n",
    "                epoch_output_dir = os.path.join(output_root_dir, f\"epoch_{epoch}\")\n",
    "                os.makedirs(epoch_output_dir, exist_ok=True)\n",
    "                for i, (images, _) in enumerate(valloader):\n",
    "                    images = images.to(device)\n",
    "                    outputs, _, _ = model(images)\n",
    "                    outputs_denorm = denormalize(outputs).cpu()\n",
    "                    for j in range(outputs_denorm.size(0)):\n",
    "                        img_tensor = outputs_denorm[j]\n",
    "                        img_pil = to_pil(img_tensor)\n",
    "                        img_pil.save(os.path.join(epoch_output_dir, f\"prediction_{i * valloader.batch_size + j}.png\"))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "input_channels = 3\n",
    "image_size = 64\n",
    "input_dim = input_channels * image_size * image_size\n",
    "vae = VAE().to(device)\n",
    "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "\n",
    "discriminator = PatchDiscriminator(input_channels=3).to(device)\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=5e-6)  # Lowered learning rate\n",
    "\n",
    "perceptual_criterion = PerceptualLoss(layer='relu3_3').to(device)\n",
    "lpips_criterion = LPIPSLoss(net='vgg').to(device)\n",
    "ssim_criterion = SSIMLoss(data_range=1.0).to(device)\n",
    "dataloader, valloader, _, _ = basic_train_config(vae, 100000, 10, 256)\n",
    "\n",
    "# Define the loss configuration with increased reconstruction weight\n",
    "loss_config = {\n",
    "    'reconstruction': {'weight': 0.5},  # Increased from 0.1\n",
    "    'kld': {'weight': 0.05},\n",
    "    'perceptual': {'weight': 0.5},\n",
    "    'lpips': {'weight': 0.5},\n",
    "    'ssim': {'weight': 0.2},\n",
    "    'gan': {'weight': 1.0},\n",
    "}\n",
    "\n",
    "\n",
    "# Train the VAE with the combined criterion\n",
    "train_vae(vae, dataloader, valloader, vae_optimizer, loss_config, discriminator, discriminator_optimizer,\n",
    "          num_epochs=25,\n",
    "          perceptual_criterion=perceptual_criterion,\n",
    "          lpips_criterion=lpips_criterion,\n",
    "          ssim_criterion=ssim_criterion,\n",
    "          run_name=\"25epoch_basic_vae_loss_balanced_stable_kld_schedule\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cb7bf9f8bb4dea9ae12e443b81aa2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=6, description='epoch_idx', max=13), Text(value='25epoch_basic_vae_loss_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_epoch(epoch_idx, run_name='default_run')>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 25 # Example\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "# Calculate the number of saved epochs\n",
    "num_saved_epochs = num_epochs // 2 if num_epochs % 2 == 0 else num_epochs // 2 + 1\n",
    "print(num_saved_epochs)\n",
    "interact(visualize_epoch, epoch_idx=(0, num_saved_epochs , 1),run_name = \"25epoch_basic_vae_loss_balanced_stable_kld_schedule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is still a problem, grid like artifacts. This is a common issue and names as \"checkerboard pattern\" and is typically caused by the use of transposed convolutions (nn.ConvTranspose2d) in the decoder, combined with the adversarial training dynamics introduced by the GAN loss. As we can see logs above, discriminator loss is too low, which indicates the discriminator can easily distinguish real images from generated ones. This imbalance makes the generator (decoder) work harder to fool the discriminator, often leading to the amplification of artifacts like the checkerboard pattern. First, we will change convtranspose2d with nn.Upsample (A common solution is to replace them with a combination of nearest-neighbor upsampling (nn.Upsample) followed by a regular convolution (nn.Conv2d).) and see if it is the main reason. After that, we will try to balance adversarial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(256, 512 * 4 * 4)  # Expanding latent vector\n",
    "\n",
    "        # Replace ConvTranspose2d with Upsample + Conv2d\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='nearest')  # 4x4 -> 8x8\n",
    "        self.conv1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(num_groups=32, num_channels=256)\n",
    "\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')  # 8x8 -> 16x16\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(num_groups=32, num_channels=128)\n",
    "\n",
    "        self.up3 = nn.Upsample(scale_factor=2, mode='nearest')  # 16x16 -> 32x32\n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.gn3 = nn.GroupNorm(num_groups=32, num_channels=64)\n",
    "\n",
    "        self.up4 = nn.Upsample(scale_factor=2, mode='nearest')  # 32x32 -> 64x64\n",
    "        self.conv4 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)  # Convert (batch_size, 256) -> (batch_size, 512 * 4 * 4)\n",
    "        z = z.view(z.size(0), 512, 4, 4)  # Reshape to 4D tensor for ConvTranspose\n",
    "\n",
    "        z = F.leaky_relu(self.gn1(self.conv1(self.up1(z))), 0.2)\n",
    "        z = F.leaky_relu(self.gn2(self.conv2(self.up2(z))), 0.2)\n",
    "        z = F.leaky_relu(self.gn3(self.conv3(self.up3(z))), 0.2)\n",
    "        pre_activation = self.conv4(self.up4(z))\n",
    "        print(\"Pre-activation (min, max, mean):\", pre_activation.min().item(), pre_activation.max().item(), pre_activation.mean().item())\n",
    "        z = torch.tanh(pre_activation)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vae(vae, dataloader, valloader, vae_optimizer, loss_config, discriminator, discriminator_optimizer,\n",
    "          num_epochs=25,\n",
    "          perceptual_criterion=perceptual_criterion,\n",
    "          lpips_criterion=lpips_criterion,\n",
    "          ssim_criterion=ssim_criterion,\n",
    "          run_name=\"25epoch_basic_vae_loss_balanced_stable_kld_schedule_updated_vae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4293119950d74cf0b3b3e29c8be51798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='epoch_idx', max=3), Text(value='25epoch_basic_vae_loss_bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_epoch(epoch_idx, run_name='default_run')>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 6 # Example\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "# Calculate the number of saved epochs\n",
    "num_saved_epochs = num_epochs // 2 if num_epochs % 2 == 0 else num_epochs // 2 + 1\n",
    "print(num_saved_epochs)\n",
    "interact(visualize_epoch, epoch_idx=(0, num_saved_epochs , 1),run_name = \"25epoch_basic_vae_loss_balanced_stable_kld_schedule_updated_vae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From first 2 epochs, we can say that problem decreased, but still appears. We will switch the decoder's group norm to layer norm, also we will add a final conv layer which aims to smooth after the last upsampling step. This can be a small 3x3 convolution with a stride of 1, and may reduce any residual high-frequency artifacts like we are facing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "\n",
    "        self.fc = nn.Linear(256, 512 * 4 * 4)  # Expanding latent vector\n",
    "\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='nearest')  # 4x4 -> 8x8\n",
    "        self.conv1 = nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln1 = nn.LayerNorm([256, 8, 8])  # Normalize over (C, H, W)\n",
    "\n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')  # 8x8 -> 16x16\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln2 = nn.LayerNorm([128, 16, 16])\n",
    "\n",
    "        self.up3 = nn.Upsample(scale_factor=2, mode='nearest')  # 16x16 -> 32x32\n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln3 = nn.LayerNorm([64, 32, 32])\n",
    "\n",
    "        self.up4 = nn.Upsample(scale_factor=2, mode='nearest')  # 32x32 -> 64x64\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.ln4 = nn.LayerNorm([64, 64, 64])\n",
    "        self.final_conv = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = self.fc(z)\n",
    "        z = z.view(z.size(0), 512, 4, 4)\n",
    "\n",
    "        z = F.leaky_relu(self.ln1(self.conv1(self.up1(z))), 0.2)\n",
    "        z = F.leaky_relu(self.ln2(self.conv2(self.up2(z))), 0.2)\n",
    "        z = F.leaky_relu(self.ln3(self.conv3(self.up3(z))), 0.2)\n",
    "        z = F.leaky_relu(self.ln4(self.conv4(self.up4(z))), 0.2)\n",
    "        pre_activation = self.final_conv(z)\n",
    "        print(\"Pre-activation (min, max, mean):\", pre_activation.min().item(), pre_activation.max().item(), pre_activation.mean().item())\n",
    "        z = torch.tanh(pre_activation)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vae(vae, dataloader, valloader, vae_optimizer, loss_config, discriminator, discriminator_optimizer,\n",
    "          num_epochs=25,\n",
    "          perceptual_criterion=perceptual_criterion,\n",
    "          lpips_criterion=lpips_criterion,\n",
    "          ssim_criterion=ssim_criterion,\n",
    "          run_name=\"25epoch_basic_vae_loss_balanced_stable_kld_schedule_updated_vae_Wlayernorm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0893d30147944488add38b3692742c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='epoch_idx', max=3), Text(value='25epoch_basic_vae_loss_bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_epoch(epoch_idx, run_name='default_run')>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 6 # Example\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "# Calculate the number of saved epochs\n",
    "num_saved_epochs = num_epochs // 2 if num_epochs % 2 == 0 else num_epochs // 2 + 1\n",
    "print(num_saved_epochs)\n",
    "interact(visualize_epoch, epoch_idx=(0, num_saved_epochs , 1),run_name = \"25epoch_basic_vae_loss_balanced_stable_kld_schedule_updated_vae_decoder_wlayernorm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train better Gan centric loss balanced training. \n",
    "\n",
    "- Increase label smoothing\n",
    "- Add gradient penalty to the discriminator. Quick note: Dicriminator can be refered as critic at other resources. \n",
    "- Decrease learning rate of discriminator, Again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSGANLoss(nn.Module):\n",
    "    def __init__(self, label_smoothing=0.3):  # Increased from 0.2\n",
    "        super(LSGANLoss, self).__init__()\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        real_target = 1.0 - self.label_smoothing  # e.g., 0.7\n",
    "        fake_target = 0.0 + self.label_smoothing  # e.g., 0.3\n",
    "        real_loss = torch.mean((real_output - real_target) ** 2)\n",
    "        fake_loss = torch.mean((fake_output - fake_target) ** 2)\n",
    "        return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "    def generator_loss(self, fake_output):\n",
    "        return torch.mean((fake_output - (1.0 - self.label_smoothing)) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=0.0005)  # Reduced from 0.001\n",
    "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-6)  # Reduced from 5e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
    "    batch_size = real_samples.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1, device=real_samples.device)\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    fake = torch.ones(d_interpolates.size(), device=real_samples.device, requires_grad=False)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# Update the training loop to include gradient penalty\n",
    "def train_vae(model, dataloader, valloader, optimizer, loss_config, discriminator=None, \n",
    "              discriminator_optimizer=None, num_epochs=20,\n",
    "              perceptual_criterion=None, lpips_criterion=None, ssim_criterion=None, \n",
    "              run_name=\"default_run\"):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    gan_loss_fn = LSGANLoss(label_smoothing=0.3)  # Increased label smoothing\n",
    "    \n",
    "    output_root_dir = f\"jn_outs/{run_name}\"\n",
    "    gt_dir = os.path.join(output_root_dir, \"ground_truth\")\n",
    "    os.makedirs(gt_dir, exist_ok=True)\n",
    "    os.makedirs(output_root_dir, exist_ok=True)\n",
    "    to_pil = ToPILImage()\n",
    "    ground_truth_saved = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, _) in enumerate(valloader):\n",
    "            if not ground_truth_saved:\n",
    "                images_denorm = denormalize(images).cpu()\n",
    "                for i in range(min(10, images_denorm.size(0))):\n",
    "                    img_tensor = images_denorm[i]\n",
    "                    img_pil = to_pil(img_tensor)\n",
    "                    img_pil.save(os.path.join(gt_dir, f\"gt_image_{batch_idx * valloader.batch_size + i}.png\"))\n",
    "                if (batch_idx + 1) * valloader.batch_size >= 10:\n",
    "                    ground_truth_saved = True\n",
    "                    break\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "        \n",
    "        kld_weight = max(0.00001, min(loss_config['kld']['weight'] * (epoch / 20.0), loss_config['kld']['weight']))\n",
    "        \n",
    "        epoch_recon_loss = 0.0\n",
    "        epoch_kld_loss = 0.0\n",
    "        epoch_perceptual_loss = 0.0\n",
    "        epoch_lpips_loss = 0.0\n",
    "        epoch_ssim_loss = 0.0\n",
    "        epoch_gan_g_loss = 0.0\n",
    "        epoch_gan_d_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        for i, (images, _) in enumerate(train_loop):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, mu, logvar = model(images)\n",
    "            print(f\"Batch {i+1} - mu (min, max, mean): {mu.min().item():.4f}, {mu.max().item():.4f}, {mu.mean().item():.4f}\")\n",
    "            print(f\"Batch {i+1} - logvar (min, max, mean): {logvar.min().item():.4f}, {logvar.max().item():.4f}, {logvar.mean().item():.4f}\")\n",
    "\n",
    "            recon_loss = reconstruction_loss(outputs, images) if 'reconstruction' in loss_config else 0.0\n",
    "            kld = kld_loss(mu, logvar) if 'kld' in loss_config else 0.0\n",
    "            perc_loss = perceptual_criterion(outputs, images) if 'perceptual' in loss_config and perceptual_criterion else 0.0\n",
    "            lpips_loss = lpips_criterion(outputs, images) if 'lpips' in loss_config and lpips_criterion else 0.0\n",
    "            ssim_loss = ssim_criterion(outputs, images) if 'ssim' in loss_config and ssim_criterion else 0.0\n",
    "\n",
    "            total_loss = combined_criterion(outputs, images, mu, logvar, loss_config, kld_weight,\n",
    "                                            perceptual_criterion=perceptual_criterion,\n",
    "                                            lpips_criterion=lpips_criterion,\n",
    "                                            ssim_criterion=ssim_criterion)\n",
    "\n",
    "            n_discriminator_steps = 1\n",
    "            g_gan_loss = 0.0\n",
    "            d_loss = 0.0\n",
    "            if 'gan' in loss_config and loss_config['gan']['weight'] > 0 and discriminator is not None:\n",
    "                for _ in range(n_discriminator_steps):\n",
    "                    discriminator_optimizer.zero_grad()\n",
    "                    real_output = discriminator(images)\n",
    "                    fake_output = discriminator(outputs.detach())\n",
    "                    d_loss = gan_loss_fn.discriminator_loss(real_output, fake_output)\n",
    "                    # Add gradient penalty\n",
    "                    gp = compute_gradient_penalty(discriminator, images, outputs.detach())\n",
    "                    d_loss += 10.0 * gp  # Weight of 10.0 for gradient penalty\n",
    "                    d_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "                    discriminator_optimizer.step()\n",
    "\n",
    "                fake_output = discriminator(outputs)\n",
    "                g_gan_loss = gan_loss_fn.generator_loss(fake_output)\n",
    "                total_loss += loss_config['gan']['weight'] * g_gan_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_recon_loss += recon_loss.item()\n",
    "            epoch_kld_loss += kld.item()\n",
    "            epoch_perceptual_loss += perc_loss.item() if perc_loss != 0 else 0.0\n",
    "            epoch_lpips_loss += lpips_loss.item() if lpips_loss != 0 else 0.0\n",
    "            epoch_ssim_loss += ssim_loss.item() if ssim_loss != 0 else 0.0\n",
    "            epoch_gan_g_loss += g_gan_loss.item() if g_gan_loss != 0 else 0.0\n",
    "            epoch_gan_d_loss += d_loss.item() if d_loss != 0 else 0.0\n",
    "            num_batches += 1\n",
    "\n",
    "            train_loop.set_postfix({\n",
    "                'Recon': f'{recon_loss.item():.4f}',\n",
    "                'KLD': f'{kld.item():.4f}',\n",
    "                'Perceptual': f'{perc_loss.item():.4f}' if perc_loss != 0 else 'N/A',\n",
    "                'LPIPS': f'{lpips_loss.item():.4f}' if lpips_loss != 0 else 'N/A',\n",
    "                'SSIM': f'{ssim_loss.item():.4f}' if ssim_loss != 0 else 'N/A',\n",
    "                'GAN_G': f'{g_gan_loss.item():.4f}' if g_gan_loss != 0 else 'N/A',\n",
    "                'GAN_D': f'{d_loss.item():.4f}' if d_loss != 0 else 'N/A',\n",
    "                'Total': f'{total_loss.item():.4f}',\n",
    "                'KLD_Weight': f'{kld_weight:.4f}'\n",
    "            })\n",
    "\n",
    "        avg_recon_loss = epoch_recon_loss / num_batches\n",
    "        avg_kld_loss = epoch_kld_loss / num_batches\n",
    "        avg_perceptual_loss = epoch_perceptual_loss / num_batches\n",
    "        avg_lpips_loss = epoch_lpips_loss / num_batches\n",
    "        avg_ssim_loss = epoch_ssim_loss / num_batches\n",
    "        avg_gan_g_loss = epoch_gan_g_loss / num_batches\n",
    "        avg_gan_d_loss = epoch_gan_d_loss / num_batches\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} - Average Losses:\")\n",
    "        print(f\"  Reconstruction: {avg_recon_loss:.4f}\")\n",
    "        print(f\"  KLD: {avg_kld_loss:.4f}\")\n",
    "        print(f\"  Perceptual: {avg_perceptual_loss:.4f}\")\n",
    "        print(f\"  LPIPS: {avg_lpips_loss:.4f}\")\n",
    "        print(f\"  SSIM: {avg_ssim_loss:.4f}\")\n",
    "        print(f\"  GAN (Generator): {avg_gan_g_loss:.4f}\")\n",
    "        print(f\"  GAN (Discriminator): {avg_gan_d_loss:.4f}\\n\")\n",
    "\n",
    "        model.eval()\n",
    "        if epoch % 2 == 0:\n",
    "            with torch.no_grad():\n",
    "                epoch_output_dir = os.path.join(output_root_dir, f\"epoch_{epoch}\")\n",
    "                os.makedirs(epoch_output_dir, exist_ok=True)\n",
    "                for i, (images, _) in enumerate(valloader):\n",
    "                    images = images.to(device)\n",
    "                    outputs, _, _ = model(images)\n",
    "                    outputs_denorm = denormalize(outputs).cpu()\n",
    "                    for j in range(outputs_denorm.size(0)):\n",
    "                        img_tensor = outputs_denorm[j]\n",
    "                        img_pil = to_pil(img_tensor)\n",
    "                        img_pil.save(os.path.join(epoch_output_dir, f\"prediction_{i * valloader.batch_size + j}.png\"))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vae(vae, dataloader, valloader, vae_optimizer, loss_config, discriminator, discriminator_optimizer,\n",
    "          num_epochs=25,\n",
    "          perceptual_criterion=perceptual_criterion,\n",
    "          lpips_criterion=lpips_criterion,\n",
    "          ssim_criterion=ssim_criterion,\n",
    "          run_name=\"25epoch_basic_vae_loss_balanced_stable_kld_schedule_no_checkerboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d84bbb9b2f4b468ce587e52d556c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='epoch_idx', max=10), Text(value='25epoch_basic_vae_loss_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_epoch(epoch_idx, run_name='default_run')>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 20 # Example\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "# Calculate the number of saved epochs\n",
    "num_saved_epochs = num_epochs // 2 if num_epochs % 2 == 0 else num_epochs // 2 + 1\n",
    "print(num_saved_epochs)\n",
    "interact(visualize_epoch, epoch_idx=(0, num_saved_epochs , 1),run_name = \"25epoch_basic_vae_loss_balanced_stable_kld_schedule_no_checkerboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, we still suffer from grid like outputs. Lets ignore GAN loss and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_config = {\n",
    "    'reconstruction': {'weight': 0.5},\n",
    "    'kld': {'weight': 0.05},\n",
    "    'perceptual': {'weight': 0.5},\n",
    "    'lpips': {'weight': 0.5},\n",
    "    'ssim': {'weight': 0.2},\n",
    "    #'gan': {'weight': 0.0},\n",
    "}\n",
    "\n",
    "train_vae(vae, dataloader, valloader, vae_optimizer, loss_config, discriminator, discriminator_optimizer,\n",
    "          num_epochs=25,\n",
    "          perceptual_criterion=perceptual_criterion,\n",
    "          lpips_criterion=lpips_criterion,\n",
    "          ssim_criterion=ssim_criterion,\n",
    "          run_name=\"25epoch_basic_vae_loss_balanced_stable_kld_schedule_no_checkerboard_no_gan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa7182a9d3a4845a9ade0d461272090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='epoch_idx', max=3), Text(value='25epoch_basic_vae_loss_bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.visualize_epoch(epoch_idx, run_name='default_run')>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 6 # Example\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "# Calculate the number of saved epochs\n",
    "num_saved_epochs = num_epochs // 2 if num_epochs % 2 == 0 else num_epochs // 2 + 1\n",
    "print(num_saved_epochs)\n",
    "interact(visualize_epoch, epoch_idx=(0, num_saved_epochs , 1),run_name = \"25epoch_basic_vae_loss_balanced_stable_kld_schedule_no_checkerboard_no_gan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And problem persists, the problem probably caused by the scale of the model, and information losses during training. For this reason, we will scale our model, also we will ad skip connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int = None, num_groups: int = 1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        out_channels = in_channels if out_channels is None else out_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=out_channels, eps=1e-6, affine=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        if self.in_channels != self.out_channels:\n",
    "            self.nin_shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        else:\n",
    "            self.nin_shortcut = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.norm1(x)\n",
    "        h = swish(h)\n",
    "        h = self.conv1(h)\n",
    "        h = self.norm2(h)\n",
    "        h = swish(h)\n",
    "        h = self.conv2(h)\n",
    "        if self.nin_shortcut is not None:\n",
    "            x = self.nin_shortcut(x)\n",
    "        return x + h\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        pad = (0, 1, 0, 1)\n",
    "        x = nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels: int):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, base_channels=64, latent_dim=256):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "        \n",
    "        # Initial convolution to map input channels to base_channels\n",
    "        self.init_conv = nn.Conv2d(3, base_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Stage 1: 64x64 -> 32x32, 64 channels\n",
    "        self.res1 = ResnetBlock(base_channels, base_channels, num_groups=32)\n",
    "        self.down1 = Downsample(base_channels)\n",
    "        \n",
    "        # Stage 2: 32x32 -> 16x16, 128 channels\n",
    "        self.res2 = ResnetBlock(base_channels, base_channels * 2, num_groups=32)\n",
    "        self.down2 = Downsample(base_channels * 2)\n",
    "        \n",
    "        # Stage 3: 16x16 -> 8x8, 256 channels\n",
    "        self.res3 = ResnetBlock(base_channels * 2, base_channels * 4, num_groups=32)\n",
    "        self.down3 = Downsample(base_channels * 4)\n",
    "        \n",
    "        # Stage 4: 8x8 -> 4x4, 512 channels\n",
    "        self.res4 = ResnetBlock(base_channels * 4, base_channels * 8, num_groups=32)\n",
    "        self.down4 = Downsample(base_channels * 8)\n",
    "        \n",
    "        # Final ResnetBlock at 4x4 resolution\n",
    "        self.res5 = ResnetBlock(base_channels * 8, base_channels * 8, num_groups=32)\n",
    "        \n",
    "        # Flatten and map to latent space\n",
    "        self.fc_mu = nn.Linear((base_channels * 8) * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear((base_channels * 8) * 4 * 4, latent_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.normal_(self.fc_mu.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.fc_logvar.weight, mean=0.0, std=0.02)\n",
    "        nn.init.zeros_(self.fc_mu.bias)\n",
    "        nn.init.zeros_(self.fc_logvar.bias)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: (B, 3, 64, 64)\n",
    "        x = self.init_conv(x)  # (B, 64, 64, 64)\n",
    "        \n",
    "        x = self.res1(x)  # (B, 64, 64, 64)\n",
    "        x = self.down1(x)  # (B, 64, 32, 32)\n",
    "        \n",
    "        x = self.res2(x)  # (B, 128, 32, 32)\n",
    "        x = self.down2(x)  # (B, 128, 16, 16)\n",
    "        \n",
    "        x = self.res3(x)  # (B, 256, 16, 16)\n",
    "        x = self.down3(x)  # (B, 256, 8, 8)\n",
    "        \n",
    "        x = self.res4(x)  # (B, 512, 8, 8)\n",
    "        x = self.down4(x)  # (B, 512, 4, 4)\n",
    "        \n",
    "        x = self.res5(x)  # (B, 512, 4, 4)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)  # (B, 512*4*4)\n",
    "        \n",
    "        # Predict mu and logvar\n",
    "        mu = self.fc_mu(x)  # (B, 256)\n",
    "        logvar = self.fc_logvar(x)  # (B, 256)\n",
    "        z = self.reparameterize(mu, logvar)  # (B, 256)\n",
    "        \n",
    "        return z, mu, logvar\n",
    "    \n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, base_channels=64, latent_dim=256):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "\n",
    "        # Map latent space to initial feature map\n",
    "        self.fc = nn.Linear(latent_dim, (base_channels * 8) * 4 * 4)\n",
    "        \n",
    "        # Stage 1: 4x4 -> 4x4, 512 channels\n",
    "        self.res1 = ResnetBlock(base_channels * 8, base_channels * 8, num_groups=32)\n",
    "        \n",
    "        # Stage 2: 4x4 -> 8x8, 512 -> 256 channels\n",
    "        self.up2 = Upsample(base_channels * 8)\n",
    "        self.res2 = ResnetBlock(base_channels * 8, base_channels * 4, num_groups=32)\n",
    "        \n",
    "        # Stage 3: 8x8 -> 16x16, 256 -> 128 channels\n",
    "        self.up3 = Upsample(base_channels * 4)\n",
    "        self.res3 = ResnetBlock(base_channels * 4, base_channels * 2, num_groups=32)\n",
    "        \n",
    "        # Stage 4: 16x16 -> 32x32, 128 -> 64 channels\n",
    "        self.up4 = Upsample(base_channels * 2)\n",
    "        self.res4 = ResnetBlock(base_channels * 2, base_channels, num_groups=32)\n",
    "        \n",
    "        # Stage 5: 32x32 -> 64x64, 64 channels\n",
    "        self.up5 = Upsample(base_channels)\n",
    "        self.res5 = ResnetBlock(base_channels, base_channels, num_groups=32)\n",
    "        \n",
    "        # Final convolution to map to 3 channels\n",
    "        self.final_conv = nn.Conv2d(base_channels, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Input: (B, 256)\n",
    "        z = self.fc(z)  # (B, 512*4*4)\n",
    "        z = z.view(z.size(0), -1, 4, 4)  # (B, 512, 4, 4)\n",
    "        \n",
    "        z = self.res1(z)  # (B, 512, 4, 4)\n",
    "        \n",
    "        z = self.up2(z)  # (B, 512, 8, 8)\n",
    "        z = self.res2(z)  # (B, 256, 8, 8)\n",
    "        \n",
    "        z = self.up3(z)  # (B, 256, 16, 16)\n",
    "        z = self.res3(z)  # (B, 128, 16, 16)\n",
    "        \n",
    "        z = self.up4(z)  # (B, 128, 32, 32)\n",
    "        z = self.res4(z)  # (B, 64, 32, 32)\n",
    "        \n",
    "        z = self.up5(z)  # (B, 64, 64, 64)\n",
    "        z = self.res5(z)  # (B, 64, 64, 64)\n",
    "        \n",
    "        pre_activation = self.final_conv(z)  # (B, 3, 64, 64)\n",
    "        print(\"Pre-activation (min, max, mean):\", pre_activation.min().item(), pre_activation.max().item(), pre_activation.mean().item())\n",
    "        z = torch.tanh(pre_activation)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, base_channels=64, latent_dim=256):\n",
    "        super().__init__()\n",
    "        self.encoder = VAE_Encoder(base_channels=base_channels, latent_dim=latent_dim)\n",
    "        self.decoder = VAE_Decoder(base_channels=base_channels, latent_dim=latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        x = self.decoder(z)\n",
    "        return x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_config = {\n",
    "    'reconstruction': {'weight': 0.5},\n",
    "    'kld': {'weight': 0.05},\n",
    "    'perceptual': {'weight': 0.5},\n",
    "    'lpips': {'weight': 0.5},\n",
    "    'ssim': {'weight': 0.2},\n",
    "    #'gan': {'weight': 0.0},\n",
    "}\n",
    "\n",
    "train_vae(vae, dataloader, valloader, vae_optimizer, loss_config, discriminator, discriminator_optimizer,\n",
    "          num_epochs=25,\n",
    "          perceptual_criterion=perceptual_criterion,\n",
    "          lpips_criterion=lpips_criterion,\n",
    "          ssim_criterion=ssim_criterion,\n",
    "          run_name=\"25epoch_basic_vae_loss_balanced_stable_kld_schedule_no_checkerboard_no_gan_scaledresnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 6 # Example\n",
    "# ... (rest of your code) ...\n",
    "\n",
    "# Calculate the number of saved epochs\n",
    "num_saved_epochs = num_epochs // 2 if num_epochs % 2 == 0 else num_epochs // 2 + 1\n",
    "print(num_saved_epochs)\n",
    "interact(visualize_epoch, epoch_idx=(0, num_saved_epochs , 1),run_name = \"25epoch_basic_vae_loss_balanced_stable_kld_schedule_no_checkerboard_no_gan_scaledresnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can use most advanced training with main.py, this notebook will be completed in the future but I believe that this is enough for you to understand whole project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
