# config/config.yaml
#
# Model Configuration Instructions
# --------------------------------
# This configuration file defines the architecture of the VAE/AE model. When modifying parameters,
# ensure the following constraints are met to avoid dimension mismatches and runtime errors:
#
# 1. **Image Size and Downsampling**:
#    - `data.image_size` must be divisible by 2^len(model.downsample_positions).
#    - Example: If `downsample_positions: [0, 1]` (2 downsamples), `image_size` must be divisible by 4.
#      For `image_size: 256`, 256 / 4 = 64, which is valid.
#    - After all downsamples, the spatial size must be >= 1.
#      Example: With 2 downsamples, 256 -> 128 -> 64 (valid).
#
# 2. **Hidden Dimensions and Block Configurations**:
#    - `model.hidden_dims` defines the channel sizes for each layer in the encoder and decoder.
#    - `model.block_configs` defines the sequence of blocks (resnet or attention).
#    - The number of blocks in the encoder/decoder is determined by `len(block_configs)`.
#    - `hidden_dims` should have enough entries to cover the blocks:
#      - If `len(block_configs) > len(hidden_dims)`, the last `hidden_dims` value is reused.
#      - Example: `hidden_dims: [64, 128, 256]` with 3 blocks works fine.
#
# 3. **Downsample Positions**:
#    - `model.downsample_positions` must be a list of indices < `len(block_configs)`.
#    - Example: With `block_configs` of length 3 (indices 0, 1, 2), `downsample_positions: [0, 1]` is valid.
#
# 4. **Latent Dimensions and VQ**:
#    - If `model.use_vq: true`, ensure `model.vq_embedding_dim` matches `model.latent_dim`.
#    - `model.vq_num_embeddings` should be large enough to represent the latent space effectively.
#
# 5. **Data Channels**:
#    - `data.data_channel` must match the input data (e.g., 3 for RGB, 1 for grayscale).
#
# 6. **Loss Configurations**:
#    - If `loss.use_perceptual`, `loss.use_lpips`, or `loss.use_ssim` is true, ensure the corresponding weights are set appropriately.
#    - `loss.reconstruction_loss` must be either "mse" or "l1".
#    - For `model.model_type: "ae"`, KL-related parameters (e.g., `beta_kl`, `kl_annealing_epochs`) are ignored.
#
# 7. **Model Type**:
#    - `model.model_type` can be "vae" (Variational Autoencoder) or "ae" (Autoencoder).
#    - "vae": Uses latent distribution with KL divergence.
#    - "ae": Directly maps input to latent space without sampling.
#
# Example Configuration:
# - `image_size: 256`, `downsample_positions: [0, 1]` (2 downsamples), `block_configs` length 3, `hidden_dims: [64, 128, 256]`.
# - This results in: 256 -> 128 -> 64 spatial sizes, with channels 64 -> 128 -> 256.
#
# If you encounter dimension mismatches, check the above constraints and ensure all parameters are consistent.
#

training:
  epochs: 20 
  batch_size: 32
  learning_rate: 1e-4
  device: "cuda"
  log_interval: 100  # logging steps
  output_dir: "./outputs"
  limit_samples: 50000 # number of samples used if dataset is too large
  n_best: 3  # Save top 3 models by validation loss

data:
  dataset_path: "/home/oguzhan/Downloads/images/"
  image_size: 64
  val_split: 0.1  # Fraction of dataset for validation
  data_channel: 3 # RGB -> 3 ; grayscale -> 1 etc.

model:
  model_type: "vae"  # Options: "vae" (Variational Autoencoder) or "ae" (Autoencoder)
  input_dim: 64
  hidden_dims: [64, 128, 256] 
  block_configs:
    - {type: "resnet"}
    - {type: "attention"}
    - {type: "resnet"}
  downsample_positions: [0, 1]  # 2 downsamples: 64 → 32 → 16
  input_size: 64
  latent_dim: 128
  vq_embedding_dim: 128
  vq_num_embeddings: 512
  vq_beta: 0.25
  use_vq: false  # Disabled as per your update
  use_ema: true
  ema_decay: 0.99
  constant_variance: false

loss:
  use_perceptual: true
  perceptual_weight: 1.0
  use_lpips: true
  lpips_weight: 1.0
  use_ssim: true
  ssm_weight: 1.0
  beta_recon: 0.5
  kl_annealing_epochs: 20
  beta_kl: 1.0 
  reconstruction_loss: "l1"  # mse or l1

gan:
  enabled: true
  discriminator_lr: 2e-4
  gan_weight: 1.0

optimize_memory_usage:
  # Enable gradient checkpointing to trade computation for memory by recomputing intermediate activations.
  use_gradient_checkpointing: true
  # Enable mixed precision training to reduce memory usage by using float16 for most computations.
  use_mixed_precision: true
  # Process perceptual losses in smaller batches to reduce peak memory usage.
  perceptual_loss_batch_size: 0  # Set to 0 to disable; otherwise, specify batch size (e.g., 4)